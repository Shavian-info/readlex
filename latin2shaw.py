import json
import csv
import re
import unidecode
import smartypants
import spacy
from spacy.util import compile_infix_regex, compile_prefix_regex, compile_suffix_regex, filter_spans
from spacy.tokens import Doc, Span
from spacy.matcher import PhraseMatcher

from bs4 import BeautifulSoup


def latin2shaw(text):
    with open("static/readlex_converter.json", 'r', encoding="utf-8") as file:
        json_data = file.read()

    readlex_dict: dict[str, list[dict[str, str]]] = json.loads(json_data)

    # Categories of letters that determine how a following 's is pronounced
    s_follows: set[str] = {"ð‘", "ð‘‘", "ð‘’", "ð‘“", "ð‘”"}
    uhz_follows: set[str] = {"ð‘•", "ð‘–", "ð‘—", "ð‘Ÿ", "ð‘ ", "ð‘¡"}
    z_follows: set[str] = {"ð‘š", "ð‘›", "ð‘œ", "ð‘", "ð‘ž", "ð‘™", "ð‘¤", "ð‘¥", "ð‘¯", "ð‘¸", "ð‘¹", "ð‘º", "ð‘»", "ð‘¼", "ð‘½"}
    consonants = set.union(s_follows, uhz_follows, z_follows)
    # vowels = {"ð‘¦", "ð‘°", "ð‘§", "ð‘±", "ð‘¨", "ð‘²", "ð‘©", "ð‘³", "ð‘ª", "ð‘´", "ð‘«", "ð‘µ", "ð‘¬", "ð‘¶", "ð‘­", "ð‘·", "ð‘¾", "ð‘¿"}
    # The following are never final other than in initialisms: "ð‘£", "ð‘¢", "ð‘˜", "ð‘®".

    # Contractions that need special treatment since the separate words are not as they appear in the dictionary
    contraction_start: dict[str, str] = {"ai": "ð‘±", "ca": "ð‘’ð‘­", "do": "ð‘›ð‘´", "does": "ð‘›ð‘³ð‘Ÿ", "did": "ð‘›ð‘¦ð‘›", "sha": "ð‘–ð‘­",
                                         "wo": "ð‘¢ð‘´",
                                         "y'": "ð‘˜"}
    contraction_end: dict[str, str] = {"n't": "ð‘¯ð‘‘", "all": "ð‘·ð‘¤", "'ve": "ð‘", "'ll": "ð‘¤", "'m": "ð‘¥", "'d": "ð‘›",
                                       "'re": "ð‘¼"}

    # Common prefixes and suffixes used in new coinings
    prefixes: dict[str, str] = {"anti": "ð‘¨ð‘¯ð‘‘ð‘¦",
                                "counter": "ð‘’ð‘¬ð‘¯ð‘‘ð‘¼",
                                "de": "ð‘›ð‘°",
                                "dis": "ð‘›ð‘¦ð‘•",
                                "esque": "ð‘§ð‘•ð‘’",
                                "hyper": "ð‘£ð‘²ð‘ð‘¼",
                                "hypo": "ð‘£ð‘²ð‘ð‘´",
                                "mega": "ð‘¥ð‘§ð‘œð‘©",
                                "meta": "ð‘¥ð‘§ð‘‘ð‘©",
                                "micro": "ð‘¥ð‘²ð‘’ð‘®ð‘´",
                                "multi": "ð‘¥ð‘³ð‘¤ð‘‘ð‘¦",
                                "mis": "ð‘¥ð‘¦ð‘•",
                                "neuro": "ð‘¯ð‘˜ð‘«ð‘¼ð‘´",
                                "non": "ð‘¯ð‘ªð‘¯",
                                "o'er": "ð‘´ð‘¼",
                                "out": "ð‘¬ð‘‘",
                                "over": "ð‘´ð‘ð‘¼",
                                "poly": "ð‘ð‘ªð‘¤ð‘¦",
                                "post": "ð‘ð‘´ð‘•ð‘‘",
                                "pre": "ð‘ð‘®ð‘°",
                                "pro": "ð‘ð‘®ð‘´",
                                "pseudo": "ð‘•ð‘¿ð‘›ð‘´",
                                "re": "ð‘®ð‘°",
                                "sub": "ð‘•ð‘³ð‘š",
                                "super": "ð‘•ð‘µð‘ð‘¼",
                                "ultra": "ð‘³ð‘¤ð‘‘ð‘®ð‘©",
                                "un": "ð‘³ð‘¯",
                                "under": "ð‘³ð‘¯ð‘›ð‘¼"
                                }
    suffixes: dict[str, str] = {"able": "ð‘©ð‘šð‘©ð‘¤",
                "bound": "ð‘šð‘¬ð‘¯ð‘›",
                "ful": "ð‘“ð‘©ð‘¤",
                "hood": "ð‘£ð‘«ð‘›",
                "ish": "ð‘¦ð‘–",
                "ism": "ð‘¦ð‘Ÿð‘©ð‘¥",
                "less": "ð‘¤ð‘©ð‘•",
                "like": "ð‘¤ð‘²ð‘’",
                "ness": "ð‘¯ð‘©ð‘•"
                }
    affixes: dict[str, str] = prefixes | suffixes

    # Words that sometimes change spelling before 'to'
    have_to: dict[str, str] = {"have": "ð‘£ð‘¨ð‘“", "has": "ð‘£ð‘¨ð‘•"}
    vbd_to: dict[str, str] = {"used": "ð‘¿ð‘•ð‘‘", "unused": "ð‘³ð‘¯ð‘¿ð‘•ð‘‘", "supposed": "ð‘•ð‘©ð‘ð‘´ð‘•ð‘‘"}
    before_to: dict[str, str] = have_to | vbd_to

    # Suffixes that follow numerals in ordinal numbers
    ordinal_suffixes: dict[str, str] = {"st": "ð‘•ð‘‘", "nd": "ð‘¯ð‘›", "rd": "ð‘®ð‘›", "th": "ð‘”", "s": "ð‘Ÿ"}

    # Load spaCy, excluding pipeline components that are not required
    nlp = spacy.load("en_core_web_sm", exclude=["parser", "lemmatizer", "textcat"])

    # Customise the spaCy tokeniser to ensure that initial and final dashes and dashes between words aren't stuck to one
    # of the surrounding words
    # Prefixes
    spacy_prefixes: list[str] = nlp.Defaults.prefixes + [r'''^[-â€“â€”]+''',]
    prefix_regex = compile_prefix_regex(spacy_prefixes)
    nlp.tokenizer.prefix_search = prefix_regex.search
    # Infixes
    spacy_infixes: list[str] = nlp.Defaults.infixes + [r'''[.,?!:;\-â€“â€”"~\(\)\[\]]+''',]
    infix_regex = compile_infix_regex(spacy_infixes)
    nlp.tokenizer.infix_finditer = infix_regex.finditer
    # Suffixes
    spacy_suffixes: list[str] = nlp.Defaults.suffixes + [r'''[-â€“â€”]+$''',]
    suffix_regex = compile_suffix_regex(spacy_suffixes)
    nlp.tokenizer.suffix_search = suffix_regex.search

    def add_span(matcher, doc, i, matches):
        match_id, start, end = matches[i]

    # Define the phrase to match
    with open("static/readlex_converter_phrases.json", "r", newline="") as f:
        reader = csv.reader(f)
        phrases = [row[0] for row in reader if row]
    phrase_patterns: list[Doc] = [nlp.make_doc(phrase) for phrase in phrases]
    phrase_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    phrase_matcher.add("phrases", phrase_patterns, on_match=add_span)

    namer_dot_ents: set[str] = {"PERSON", "FAC", "ORG", "GPE", "LOC", "PRODUCT", "EVENT", "WORK_OF_ART", "LAW"}

    def tokenise(text: str) -> spacy.tokens.Doc:
        # Tokenise and tag the text using spaCy as doc

        doc = nlp(text)
        phrase_matches = phrase_matcher(doc)
        phrase_spans: list[Span] = []
        for match_id, start, end in phrase_matches:
            span = Span(doc, start, end, label=match_id)
            phrase_spans.append(span)

        filtered_spans = filter_spans(phrase_spans)

        with doc.retokenize() as retokenizer:
            for span in filtered_spans:
                retokenizer.merge(span)

        # Expand person entities to include titles and take initial 'the' out of entity names
        titles: set[str] = {
            "archbishop",
            "archdeacon",
            "baron",
            "baroness",
            "bishop",
            "captain",
            "count",
            "countess",
            "cpt",
            "dame",
            "deacon",
            "doctor",
            "dr.",
            "dr",
            "duchess",
            "duke",
            "earl",
            "emperor",
            "empress",
            "gov.",
            "gov",
            "governor",
            "justice",
            "king",
            "lady",
            "lord",
            "marchioness",
            "marquess",
            "marquis",
            "miss",
            "missus",
            "mister",
            "mistress",
            "mr.",
            "mr",
            "mrs.",
            "mrs",
            "ms.",
            "ms",
            "mx.",
            "mx",
            "pope",
            "pres.",
            "pres",
            "president",
            "prince",
            "princess",
            "prof.",
            "prof",
            "professor",
            "queen",
            "rev.",
            "rev",
            "reverend",
            "saint",
            "sen.",
            "sen",
            "senator",
            "sir",
            "st.",
            "st",
            "viscount",
            "viscountess"
        }
        new_ents: list[Span] = []
        for ent in doc.ents:
            # Only check for title if it's a person and not the first token
            if ent.label_ == "PERSON" and ent.start != 0:
                prev_token = doc[ent.start - 1]
                if prev_token.lower_ in titles:
                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)
                    new_ents.append(new_ent)
                else:
                    new_ents.append(ent)
            elif ent.label_ in namer_dot_ents:
                if doc[ent.start].lower_ == "the":
                    new_ent = Span(doc, ent.start + 1, ent.end, label=ent.label)
                    new_ents.append(new_ent)
                else:
                    new_ents.append(ent)
            else:
                new_ents.append(ent)

        filtered_ents = filter_spans(new_ents)
        doc.ents = tuple(filtered_ents)

        return doc

    def convert(doc: spacy.tokens.Doc) -> str:
        # Apply a series of tests to each token to determine how to Shavianise it.
        text_split_shaw: str = ""

        for token in doc:

            # Leave HTML tags unchanged
            if token.tag_ == "HTML":
                text_split_shaw += token.text

            # Convert contractions
            if token.lower_ in contraction_start and token.i < len(doc) - 1 and doc[
                token.i + 1].lower_ in contraction_end:
                text_split_shaw += contraction_start[token.lower_]
            elif token.lower_ in contraction_end:
                prefix: str = "ð‘©" if token.lower_ != "ð‘¼" and text_split_shaw and text_split_shaw[
                    -1] in consonants else ""
                text_split_shaw += prefix + contraction_end[token.lower_] + token.whitespace_

            # Convert possessive 's
            elif token.lower_ == "'s":
                suffix: str = "ð‘•" if text_split_shaw[-1] in s_follows else "ð‘©ð‘Ÿ" if text_split_shaw[
                                                                                       -1] in uhz_follows else "ð‘Ÿ"
                text_split_shaw += suffix + token.whitespace_

            # Convert possessive '
            elif token.lower_ == "'" and token.tag_ == "POS":
                text_split_shaw += token.whitespace_

            # Convert verbs that change pronunciation before 'to', e.g. 'have to', 'used to', 'supposed to'
            elif token.lower_ in before_to and token.i < len(doc) - 1 and doc[token.i + 1].lower_ == "to":
                # 'have' only changes pronunciation where 'have to' means 'must'
                if token.lower_ in have_to and doc[token.i + 2].tag_ in ["VB", "VBP"]:
                    text_split_shaw += have_to[token.lower_] + token.whitespace_
                # 'used', 'supposed' etc. only change pronunciation in the past tense, not past participle
                elif token.lower_ in vbd_to and token.tag_ in ["VBD", "VBN", "."]:
                    text_split_shaw += vbd_to[token.lower_] + token.whitespace_

            # Match ordinal numbers represented by a numeral and a suffix
            elif re.fullmatch(r"([0-9]+(?:[, .]?[0-9]+)*)(st|nd|rd|th|s)", token.lower_):
                number, number_suffix = re.match(r"([0-9]+(?:[, .]?[0-9]+)*)(st|nd|rd|th|s)", token.lower_).groups()
                text_split_shaw += number + ordinal_suffixes[number_suffix] + token.whitespace_

            # Loop through the words in the ReadLex and look for matches, and only apply the namer dot to the first word
            # in a name (or not at all for initialisms marked with â¸°)
            elif token.lower_ in readlex_dict:
                for i in readlex_dict.get(token.lower_, []):
                    # Match the part of speech for heteronyms
                    if i["tag"] == token.tag_:
                        prefix: str = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in namer_dot_ents and not i[
                            "Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

                    # For any proper nouns not in the ReadLex, match if an identical common noun exists
                    elif (i["tag"] in ["NN", "0"] and token.tag_ == "NNP") or (
                            i["tag"] in ["NNS", "0"] and token.tag_ == "NNPS"):
                        prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in namer_dot_ents and not i[
                            "Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

                    # Match words with only one pronunciation
                    elif i["tag"] == "0":
                        prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in namer_dot_ents and not i[
                            "Shaw"].startswith("â¸°") else ""
                        text_split_shaw += prefix + i["Shaw"] + token.whitespace_
                        break

            # Apply additional tests where there is still no match
            else:
                found: bool = False
                constructed_warning: str = "âš ï¸"
                '''
                Try to construct a match using common prefixes and suffixes and include a warning symbol to aid proof
                reading
                '''
                for j in affixes:
                    if token.lower_.startswith(j) and j in prefixes:
                        prefix: str = prefixes[j]
                        suffix: str = ""
                        target_word: str = token.lower_[len(j):]
                    elif token.lower_.endswith(j) and j in suffixes:
                        prefix = ""
                        suffix = suffixes[j]
                        target_word = token.lower_[:-len(j)]
                    else:
                        continue
                    if target_word in readlex_dict:
                        found = True
                        for i in readlex_dict.get(target_word):
                            prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in namer_dot_ents and not \
                                i[
                                    "Shaw"].startswith("â¸°") else prefix
                            text_split_shaw += prefix + i[
                                "Shaw"] + suffix + constructed_warning + token.whitespace_
                            break

                # Try to construct plurals if not expressly included in the ReadLex, e.g. plurals of proper names.
                if token.lower_.endswith("s"):
                    target_word = token.lower_[:-1]
                    if target_word in readlex_dict:
                        found = True
                        for i in readlex_dict.get(target_word):
                            suffix = "ð‘•" if i["Shaw"][-1] in s_follows else "ð‘©ð‘Ÿ" if i["Shaw"][
                                                                                        -1] in uhz_follows else "ð‘Ÿ"
                            prefix = "Â·" if token.ent_iob_ == "B" and token.ent_type_ in namer_dot_ents and not \
                                i[
                                    "Shaw"].startswith("â¸°") else ""
                            text_split_shaw += prefix + i[
                                "Shaw"] + suffix + constructed_warning + token.whitespace_
                            break

                if found is not False:
                    continue
                # If there is still no match, do not convert the word
                if token.text.isalpha():
                    text_split_shaw += token.text + "âœ¢" + token.whitespace_
                else:
                    text_split_shaw += token.text + token.whitespace_

        return text_split_shaw

    # Create the string that will contain the Shavianised text.
    text_shaw: str = ""

    # Split up the string to reduce the risk of spaCy exceeding memory limits
    if text.strip().casefold().startswith("<!doctype html"):
        style_pattern: str = r"(<style\b[^>]*>.*?</style>)"
        script_pattern: str = r"(<script\b[^>]*>.*?</script>)"
        html_pattern: str = r"(?!(?:<style[^>]*?>.*?</style>|<script[^>]*?>.*?</script>))(<.*?>)"
        html_patterns: str = f"{style_pattern}|{script_pattern}|{html_pattern}"
        text_split: list[str] = re.split(html_patterns, text, flags=re.DOTALL)
        for text_part in text_split:
            if text_part is None:
                pass
            elif re.fullmatch(style_pattern, text_part, flags=re.DOTALL) or re.fullmatch(
                    script_pattern, text_part, flags=re.DOTALL) or re.fullmatch(html_pattern, text_part,
                                                                                flags=re.DOTALL):
                text_shaw += text_part
            else:
                doc: spacy.tokens.Doc = tokenise(text_part)
                text_shaw += convert(doc)

        # Convert dumb quotes, double hyphens, etc. to their typographic equivalents
        text_shaw = smartypants.smartypants(text_shaw)
        # Convert curly quotes to angle quotes
        quotation_marks: dict[str, str] = {"&#8216;": "&lsaquo;", "&#8217;": "&rsaquo;", "&#8220;": "&laquo;", "&#8221;": "&raquo;"}
        for key, value in quotation_marks.items():
            text_shaw = text_shaw.replace(key, value)

    else:
        text = unidecode.unidecode(text)
        text = re.sub(r"(\S)(\[)", r"\1 \2", text)
        text = re.sub(r"](\S)", r"] \1", text)
        text_split: list[str] = text.splitlines()
        for i in text_split:
            if len(i) < 10000:
                doc: spacy.tokens.Doc = tokenise(i)
                text_shaw += convert(doc) + "\n"
        # Convert dumb quotes, double hyphens, etc. to their typographic equivalents
        text_shaw = smartypants.smartypants(text_shaw)
        quotation_marks: dict[str, str] = {"&#8216;": "&lsaquo;", "&#8217;": "&rsaquo;", "&#8220;": "&laquo;", "&#8221;": "&raquo;"}
        for key, value in quotation_marks.items():
            text_shaw = text_shaw.replace(key, value)
        text_shaw = str(BeautifulSoup(text_shaw, features="html.parser"))

    return text_shaw